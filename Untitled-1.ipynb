{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import pathlib\n",
    "import urllib.parse\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from shapely.geometry import shape\n",
    "\n",
    "import sqlalchemy as db\n",
    "import sqlalchemy.dialects.postgresql as pg\n",
    "import hashlib\n",
    "\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from geoalchemy2.types import Geometry\n",
    "from geoalchemy2.shape import from_shape\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "from shapely.wkb import loads as wkb_loads\n",
    "import geoplot as gplt\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "# data prepare\n",
    "# Where data files will be read from/written to - this should already exist\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "\n",
    "# ZIPCODE_DATA_FILE = DATA_DIR / \"zipcodes\" / \"ZIP_CODE_040114.shp\"\n",
    "ZIPCODE_DATA_FILE = DATA_DIR / \"zipcodes\" / \"nyc_zipcodes.shp\"\n",
    "\n",
    "ZILLOW_DATA_FILE = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "\n",
    "NYC_DATA_APP_TOKEN = \"EqSebYPKeoZPWmssyC2rvIPN8\"\n",
    "BASE_NYC_DATA_URL = \"https://data.cityofnewyork.us/\"\n",
    "NYC_DATA_311 = \"erm2-nwe9.geojson\"\n",
    "NYC_DATA_TREES = \"5rq2-4hqu.geojson\"\n",
    "\n",
    "DB_NAME = \"apartment\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_URL = f\"postgresql+psycopg2://{DB_USER}@localhost/{DB_NAME}\"\n",
    "DB_SCHEMA_FILE = \"schema.sql\"\n",
    "# directory where DB queries for Part 3 will be saved\n",
    "QUERY_DIR = pathlib.Path(\"queries\")\n",
    "\n",
    "BASIC_USER = 'bo8yv64rbrt1cas4iyua598vp'\n",
    "BASIC_PASS = '5vwh31bomglif6wi66lb1py390txqu57vkgv8319f2kg1hxkuk'\n",
    "\n",
    "\n",
    "# When FLAG_DEBUG == True, record size will be limited to 100,000\n",
    "FLAG_DEBUG = False\n",
    "\n",
    "# Make sure the QUERY_DIRECTORY exists\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avoid_nan(value):\n",
    "    \"\"\"Replace NaN in a cell with None to avoid errors when saving to the database\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "def debug_warp(count:int):\n",
    "    \"\"\"When FLAG_DEBUG is True, limit record size to 10000\"\"\"\n",
    "    if FLAG_DEBUG:\n",
    "        return 100000\n",
    "    else:\n",
    "        return count\n",
    "\n",
    "\n",
    "def build_query_url(base_url: str, queries: dict = {}, flag_use_token: bool = True):\n",
    "    \"\"\"Build queries into url query string, and add api token to url\"\"\"\n",
    "    result = base_url[:] + \"?\"\n",
    "\n",
    "    if flag_use_token:\n",
    "        result += f\"$$app_token={NYC_DATA_APP_TOKEN}&\"\n",
    "\n",
    "    for key in queries:\n",
    "        result += f\"{key}={queries[key]}&\"\n",
    "\n",
    "    return result[:-1]\n",
    "\n",
    "\n",
    "def get_md5(content: str):\n",
    "    \"\"\"Calculate the md5 of a string\"\"\"\n",
    "    if isinstance(content, str):\n",
    "        content = content.encode()\n",
    "    md5 = hashlib.md5()\n",
    "    md5.update(content)\n",
    "    return md5.hexdigest()\n",
    "\n",
    "\n",
    "def get_with_cache(url: str, update: bool = False):\n",
    "    \"\"\"This function implements a get function with Cache\"\"\"\n",
    "    url_md5 = get_md5(url)\n",
    "    storage_path = DATA_DIR / url_md5\n",
    "\n",
    "    print('update or (not storage_path.exists()):',\n",
    "          update or (not storage_path.exists()))\n",
    "\n",
    "    if update or (not storage_path.exists()):\n",
    "        print(f\"Downloading {url} ...\")\n",
    "\n",
    "        session = requests.Session()\n",
    "        # session.headers.update({\"X-App-Token\": NYC_DATA_APP_TOKEN})\n",
    "        basic = requests.auth.HTTPBasicAuth(BASIC_USER, BASIC_PASS)\n",
    "\n",
    "        count = 5\n",
    "\n",
    "        while count >= 0:\n",
    "            response = None\n",
    "            print(f'Download try: {5 + 1 - count}')\n",
    "            try:\n",
    "                response = session.get(url, auth=basic)\n",
    "            except Exception:\n",
    "                print('Network error, retry')\n",
    "                count -= 1\n",
    "                continue\n",
    "\n",
    "            if response:\n",
    "                with open(storage_path, \"wb\") as file_handle:\n",
    "                    file_handle.write(response.content)\n",
    "                print(f\"Done downloading {url}.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Download {url} fail, reason:\",\n",
    "                    response.status_code,\n",
    "                    \"message:\",\n",
    "                    response.content.decode(),\n",
    "                )\n",
    "                continue\n",
    "\n",
    "    return storage_path\n",
    "\n",
    "\n",
    "def download_nyc_geojson_data(url: str, force: bool = False):\n",
    "    \"\"\"This function is deprecated because it doesn't support url with query's very well\"\"\"\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    url_path = parsed_url.path.strip(\"/\")\n",
    "\n",
    "    filename = DATA_DIR / url_path\n",
    "\n",
    "    if force or not filename.exists():\n",
    "        print(f\"Downloading {url} to {filename}...\")\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response:\n",
    "            with open(filename, \"w\") as file_handle:\n",
    "                # json.dump(..., f)\n",
    "                file_handle.write(response.content)\n",
    "            print(f\"Done downloading {url}.\")\n",
    "        else:\n",
    "            print(f\"Download {url} fail.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Reading from {filename}...\")\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zipcodes(zipcode_datafile) :\n",
    "    \"\"\"Loading and cleaning the zip code dataset\"\"\"\n",
    "\n",
    "    gdf = gpd.read_file(zipcode_datafile)\n",
    "\n",
    "    mapping_k = ['ZIPCODE', 'PO_NAME', 'STATE', 'COUNTY', 'geometry']\n",
    "\n",
    "    mapping_v = ['zipcode', 'neighborhood', 'state', 'county', 'geometry']\n",
    "\n",
    "    name_mapping = dict(map(lambda value_key, value_value: (value_key, value_value), mapping_k, mapping_v))\n",
    "\n",
    "    gdf.crs = 'epsg:2263'\n",
    "\n",
    "    gdf = gdf.to_crs('epsg:4326')\n",
    "\n",
    "    # result = pd.DataFrame(gdf)\n",
    "    result = gdf\n",
    "\n",
    "    column_to_delete = []\n",
    "\n",
    "    for column_name in result.columns:\n",
    "\n",
    "        if column_name not in mapping_k:\n",
    "\n",
    "            column_to_delete.append(column_name)\n",
    "\n",
    "    result = result.drop(column_to_delete, axis=1)\n",
    "\n",
    "    result = result.rename(columns=name_mapping)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_and_clean_311_data():\n",
    "    \"\"\"Loading and Cleaning 311 Complaint Data Set\"\"\"\n",
    "    # https://data.cityofnewyork.us/resource/erm2-nwe9.json\n",
    "\n",
    "    params = {\n",
    "        '$select': 'count(unique_key)',\n",
    "        # '$where': 'complaint_type LIKE \"%25Noise%25\" AND created_date >= \"2022-01-01T00:00:00\"::floating_timestamp',\n",
    "        '$where': 'created_date >= \"2022-01-01T00:00:00\"::floating_timestamp',\n",
    "    }\n",
    "\n",
    "    query_url = build_query_url(\n",
    "        'https://data.cityofnewyork.us/resource/erm2-nwe9.json', params, False)\n",
    "\n",
    "    print('Will call get_with_cache() ... 1')\n",
    "    dataset_path = get_with_cache(query_url, update=False)\n",
    "    print('     Call get_with_cache() ... 1 ... end')\n",
    "\n",
    "    jsonStr = '[]'\n",
    "\n",
    "    with open(dataset_path, 'rb') as file_handle:\n",
    "        jsonStr = file_handle.read().decode()\n",
    "\n",
    "    row_count_since_20220101 = debug_warp(\n",
    "        int(json.loads(jsonStr)[0]['count_unique_key']))\n",
    "\n",
    "    query_url = build_query_url(\n",
    "        'https://data.cityofnewyork.us/resource/erm2-nwe9.json', params, False)\n",
    "\n",
    "    collected_row_count = 0\n",
    "\n",
    "    result = gpd.GeoDataFrame()\n",
    "\n",
    "    print(f'Report {collected_row_count} / {row_count_since_20220101} ...')\n",
    "\n",
    "    while collected_row_count < row_count_since_20220101:\n",
    "\n",
    "        print(\n",
    "            f'Collect {collected_row_count} / {row_count_since_20220101} ...')\n",
    "\n",
    "        params = {\n",
    "            '$select': 'unique_key, created_date, complaint_type, incident_zip, latitude, longitude',\n",
    "            # '$where': 'complaint_type LIKE \"%25Noise%25\" AND created_date >= \"2022-01-01T00:00:00\"::floating_timestamp',\n",
    "            '$where': 'created_date >= \"2022-01-01T00:00:00\"::floating_timestamp',\n",
    "            '$limit': '150000',\n",
    "            '$offset': collected_row_count\n",
    "        }\n",
    "\n",
    "        query_url = build_query_url(\n",
    "            'https://data.cityofnewyork.us/resource/erm2-nwe9.json', params, False)\n",
    "\n",
    "        print('Will call get_with_cache() ... 2')\n",
    "        dataset_path = get_with_cache(query_url)\n",
    "\n",
    "        jsonStr = '[]'\n",
    "\n",
    "        with open(dataset_path, 'rb') as file_handle:\n",
    "            jsonStr = file_handle.read().decode()\n",
    "\n",
    "        jsonObject = json.loads(jsonStr)\n",
    "\n",
    "        part_dataframe = pd.DataFrame.from_records(jsonObject)\n",
    "\n",
    "        result = pd.concat([result, part_dataframe], ignore_index=True)\n",
    "\n",
    "        collected_row_count += len(jsonObject)\n",
    "\n",
    "    result['incident_zip'].fillna(value=-1, inplace=True)\n",
    "    result['geometry'] = gpd.points_from_xy(\n",
    "        result['longitude'], result['latitude'])\n",
    "    result = result.drop(['latitude', 'longitude'], axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_clean_tree_data():\n",
    "    \"\"\"Load and clean the tree dataset\"\"\"\n",
    "\n",
    "    # https://data.cityofnewyork.us/resource/5rq2-4hqu.json\n",
    "\n",
    "    # [\"tree_id\", \"spc_common\", \"zipcode\", \"status\", \"the_geom\"]\n",
    "\n",
    "    params = {\n",
    "        \"$select\": \"count(tree_id)\"\n",
    "    }\n",
    "\n",
    "    query_url = build_query_url(\n",
    "        \"https://data.cityofnewyork.us/resource/5rq2-4hqu.json\", params, False\n",
    "    )\n",
    "\n",
    "    dataset_path = get_with_cache(query_url, update=False)\n",
    "\n",
    "    jsonStr = \"[]\"\n",
    "\n",
    "    with open(dataset_path, \"rb\") as file_handle:\n",
    "\n",
    "        jsonStr = file_handle.read().decode()\n",
    "\n",
    "    row_count = debug_warp(int(json.loads(jsonStr)[0][\"count_tree_id\"]))\n",
    "\n",
    "    collected_row_count = 0\n",
    "\n",
    "    result = gpd.GeoDataFrame()\n",
    "\n",
    "    while collected_row_count < row_count:\n",
    "\n",
    "        print(f\"Collect {collected_row_count} / {row_count} ...\")\n",
    "\n",
    "        params = {\n",
    "            \"$select\": \"tree_id, spc_common, zipcode, status, health, the_geom\",\n",
    "            \"$limit\": \"150000\",\n",
    "            \"$offset\": collected_row_count,\n",
    "        }\n",
    "\n",
    "        query_url = build_query_url(\n",
    "            \"https://data.cityofnewyork.us/resource/5rq2-4hqu.json\", params, False\n",
    "        )\n",
    "\n",
    "        dataset_path = get_with_cache(query_url)\n",
    "\n",
    "        jsonStr = \"[]\"\n",
    "\n",
    "        with open(dataset_path, \"rb\") as file_handle:\n",
    "\n",
    "            jsonStr = file_handle.read().decode()\n",
    "\n",
    "        jsonObject = json.loads(jsonStr)\n",
    "\n",
    "        part_dataframe = pd.DataFrame.from_records(jsonObject)\n",
    "\n",
    "        result = pd.concat([result, part_dataframe], ignore_index=True)\n",
    "\n",
    "        collected_row_count += len(jsonObject)\n",
    "\n",
    "    result = result.rename(columns={\"the_geom\": \"geometry\"})\n",
    "\n",
    "    result['geometry'] = result['geometry'].apply(shape)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zillow_data():\n",
    "    \"\"\"Load and clean historical rental dataset\"\"\"\n",
    "\n",
    "    df = pd.read_csv(ZILLOW_DATA_FILE)\n",
    "\n",
    "    df = df.drop(['RegionID', 'SizeRank', 'RegionType', 'StateName',\n",
    "                 'City', 'Metro', 'CountyName'], axis=1)\n",
    "\n",
    "    df = df.rename(columns={'RegionName': 'zipcode'})\n",
    "\n",
    "    columns = df.columns\n",
    "\n",
    "    columns = columns[2:]\n",
    "\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    values = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        for column in columns:\n",
    "\n",
    "            new_row = {\n",
    "\n",
    "                'zipcode': row['zipcode'],\n",
    "\n",
    "                'state': row['State'],\n",
    "\n",
    "                'date': column,\n",
    "\n",
    "                'average_price': row[column]\n",
    "\n",
    "            }\n",
    "\n",
    "            values.append(new_row)\n",
    "\n",
    "    result = pd.DataFrame.from_records(values)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    \"\"\"Load all data\"\"\"\n",
    "\n",
    "    geodf_zipcode_data = load_and_clean_zipcodes(ZIPCODE_DATA_FILE)\n",
    "\n",
    "    geodf_311_data = download_and_clean_311_data()\n",
    "\n",
    "    geodf_tree_data = download_and_clean_tree_data()\n",
    "\n",
    "    df_zillow_data = load_and_clean_zillow_data()\n",
    "\n",
    "    return (geodf_zipcode_data, geodf_311_data, geodf_tree_data, df_zillow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
